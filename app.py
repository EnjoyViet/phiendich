import streamlit as st
import whisper
import google.generativeai as genai
from gtts import gTTS
import io
import tempfile
import base64  # JS ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ìš©

# ì–¸ì–´ ë§¤í•‘ (5ê°œ ì–¸ì–´)
LANGUAGES = {'í•œêµ­ì–´': 'ko', 'ë² íŠ¸ë‚¨ì–´': 'vi', 'ì˜ì–´': 'en', 'ì¤‘êµ­ì–´': 'zh', 'ì¼ë³¸ì–´': 'ja'}
LANG_NAMES = list(LANGUAGES.keys())

st.title("ë™ì‹œ í†µì—­ê¸° (ì‹¤ì‹œê°„ ë§ˆì´í¬: ë§í•˜ë©´ ìë™ ì¸ì‹ â†’ ë²ˆì—­ â†’ ìŒì„± ì¶œë ¥)")

# API í‚¤ ì…ë ¥ UI
if 'api_key' not in st.session_state:
    st.session_state.api_key = ""

api_key_input = st.text_input("Google API í‚¤ ì…ë ¥ (https://aistudio.google.com/api-keysì—ì„œ ë°œê¸‰)", 
                              value=st.session_state.api_key, 
                              type="password")

if api_key_input:
    st.session_state.api_key = api_key_input
    API_KEY = api_key_input
else:
    st.warning("API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”! (Gemini ë²ˆì—­ì— í•„ìš”)")
    st.stop()

# Google Gemini API ì„¤ì •
genai.configure(api_key=API_KEY)
gemini_model = genai.GenerativeModel('gemini-1.5-flash')

# Whisper ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_whisper():
    return whisper.load_model("base")

whisper_model = load_whisper()

# ê¸°ë³¸ê°’ ì„¤ì •
if 'input_lang' not in st.session_state:
    st.session_state.input_lang = 'í•œêµ­ì–´'
if 'output_lang' not in st.session_state:
    st.session_state.output_lang = 'ë² íŠ¸ë‚¨ì–´'

# ì–¸ì–´ ì„ íƒ
input_lang = st.selectbox("ì…ë ¥ ì–¸ì–´", LANG_NAMES, index=LANG_NAMES.index(st.session_state.input_lang), key='input_select')
output_lang = st.selectbox("ì¶œë ¥ ì–¸ì–´", LANG_NAMES, index=LANG_NAMES.index(st.session_state.output_lang), key='output_select')

st.session_state.input_lang = input_lang
st.session_state.output_lang = output_lang

# ì–¸ì–´ ì „í™˜ ë²„íŠ¼
if st.button("ì–¸ì–´ ì „í™˜ (A â†” B)"):
    temp = st.session_state.input_lang
    st.session_state.input_lang = st.session_state.output_lang
    st.session_state.output_lang = temp
    st.rerun()

# ì‹¤ì‹œê°„ ë§ˆì´í¬ ë…¹ìŒ (JS ì»´í¬ë„ŒíŠ¸ â€“ ë¸Œë¼ìš°ì € ë§ˆì´í¬ ì§ì ‘ ì‚¬ìš©)
st.write("ğŸ¤ ë§ˆì´í¬ ë²„íŠ¼ìœ¼ë¡œ ë§í•˜ì„¸ìš” (5ì´ˆ ìë™ ë…¹ìŒ í›„ ì²˜ë¦¬). ìŠ¤ë§ˆíŠ¸í°ì—ì„œ ë§ˆì´í¬ ê¶Œí•œ í—ˆìš©í•˜ì„¸ìš”!")
if 'recorded_audio' not in st.session_state:
    st.session_state.recorded_audio = None

# JSë¡œ ë§ˆì´í¬ ë…¹ìŒ (Web Audio API â€“ í´ë¼ìš°ë“œ í˜¸í™˜)
mic_js = """
<div id="mic-div">
    <button id="mic-btn" onclick="toggleMic()">ğŸ¤ ë§í•˜ê¸° ì‹œì‘</button>
    <p id="status">ì¤€ë¹„ ì¤‘... (ë§ˆì´í¬ í—ˆìš© í›„ í´ë¦­)</p>
</div>
<script>
let isRecording = false;
let mediaRecorder;
let audioChunks = [];

async function toggleMic() {
    const status = document.getElementById('status');
    const btn = document.getElementById('mic-btn');
    if (!isRecording) {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            mediaRecorder = new MediaRecorder(stream);
            audioChunks = [];
            isRecording = true;
            btn.innerHTML = 'â¹ï¸ ì¤‘ì§€';
            status.textContent = 'ë…¹ìŒ ì¤‘... (5ì´ˆ í›„ ìë™ ì²˜ë¦¬)';
            
            mediaRecorder.ondataavailable = (e) => audioChunks.push(e.data);
            mediaRecorder.onstop = async () => {
                const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                const reader = new FileReader();
                reader.readAsDataURL(audioBlob);
                reader.onloadend = () => {
                    // Streamlit ì„¸ì…˜ì— ë°ì´í„° ì €ì¥
                    parent.window.streamlitSetComponentValue({audio: reader.result});
                };
                stream.getTracks().forEach(track => track.stop());
            };
            mediaRecorder.start();
            setTimeout(() => {
                if (isRecording) mediaRecorder.stop();
            }, 5000);  # 5ì´ˆ ìë™ ì¤‘ì§€
        } catch (err) {
            status.textContent = 'ë§ˆì´í¬ ì˜¤ë¥˜: ' + err.message;
        }
    } else {
        mediaRecorder.stop();
        isRecording = false;
        btn.innerHTML = 'ğŸ¤ ë§í•˜ê¸° ì‹œì‘';
        status.textContent = 'ì²˜ë¦¬ ì¤‘...';
    }
}
</script>
"""

# Streamlit ì»´í¬ë„ŒíŠ¸ë¡œ JS ì„ë² ë“œ
st.components.v1.html(mic_js, height=100)

# ë…¹ìŒ ë°ì´í„° ì²˜ë¦¬ (JSì—ì„œ ì „ì†¡ëœ ì˜¤ë””ì˜¤)
recorded_audio = st.session_state.get('recorded_audio')
if recorded_audio and 'audio' in recorded_audio:
    audio_base64 = recorded_audio['audio']
    # Base64ë¥¼ ë°”ì´ë„ˆë¦¬ë¡œ ë³€í™˜
    audio_bytes = base64.b64decode(audio_base64.split(',')[1])
    
    # ì„ì‹œ íŒŒì¼ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
        tmp_file.write(audio_bytes)
        tmp_path = tmp_file.name
    
    # STT: ìŒì„± â†’ í…ìŠ¤íŠ¸
    result = whisper_model.transcribe(tmp_path, language=LANGUAGES[input_lang])
    text = result["text"].strip()
    st.write(f"ì¸ì‹ëœ í…ìŠ¤íŠ¸ ({input_lang}): {text}")
    
    if not text:
        st.warning("ìŒì„±ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
    else:
        # Gemini AI ë²ˆì—­
        prompt = f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {output_lang}ë¡œ ì „ë¬¸ í†µì—­ì‚¬ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•˜ê²Œ ë²ˆì—­í•˜ì„¸ìš”. 
        êµ¬ì–´ì²´ë¥¼ ìœ ì§€í•˜ë©°, ë¬¸í™”ì  ë§¥ë½ê³¼ ë‰˜ì•™ìŠ¤ë¥¼ ê³ ë ¤í•˜ì„¸ìš”. ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.
        
        ì›ë¬¸ ({input_lang}): {text}
        """
        
        response = gemini_model.generate_content(prompt)
        translated_text = response.text.strip()
        st.write(f"ë²ˆì—­ ê²°ê³¼ ({output_lang}) - Gemini AI: {translated_text}")
        
        # TTS: í…ìŠ¤íŠ¸ â†’ ìŒì„± ì¶œë ¥ (ì—¬ì„±-like, ì¹œê·¼)
        tts = gTTS(translated_text, lang=LANGUAGES[output_lang], slow=False)
        audio_file = io.BytesIO()
        tts.write_to_fp(audio_file)
        audio_file.seek(0)
        st.audio(audio_file, format='audio/mp3')
        st.success("ë™ì‹œ í†µì—­ ì™„ë£Œ! (ë§í•˜ë©´ ìë™ ì²˜ë¦¬ëì–´ìš”.)")
    
    st.session_state.recorded_audio = None

# íŒŒì¼ ì—…ë¡œë“œ ëŒ€ì•ˆ (ì„ì‹œ â€“ ë“¤ì—¬ì“°ê¸° ì™„ë²½ ê³ ì¹¨)
uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ (ëŒ€ì•ˆ)", type=['wav', 'mp3', 'm4a'])
if uploaded_file is not None:
    st.write("íŒŒì¼ ì—…ë¡œë“œëì–´ìš”! ì²˜ë¦¬ ì¤‘...")
    
    # ì„ì‹œ íŒŒì¼ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_path = tmp_file.name
    
    # STT: ìŒì„± â†’ í…ìŠ¤íŠ¸
    result = whisper_model.transcribe(tmp_path, language=LANGUAGES[input_lang])
    text = result["text"].strip()
    st.write(f"ì¸ì‹ëœ í…ìŠ¤íŠ¸ ({input_lang}): {text}")
    
    if not text:
        st.warning("ìŒì„±ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
    else:
        # Gemini AI ë²ˆì—­
        prompt = f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {output_lang}ë¡œ ì „ë¬¸ í†µì—­ì‚¬ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•˜ê²Œ ë²ˆì—­í•˜ì„¸ìš”. 
        êµ¬ì–´ì²´ë¥¼ ìœ ì§€í•˜ë©°, ë¬¸í™”ì  ë§¥ë½ê³¼ ë‰˜ì•™ìŠ¤ë¥¼ ê³ ë ¤í•˜ì„¸ìš”. ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.
        
        ì›ë¬¸ ({input_lang}): {text}
        """
        
        response = gemini_model.generate_content(prompt)
        translated_text = response.text.strip()
        st.write(f"ë²ˆì—­ ê²°ê³¼ ({output_lang}) - Gemini AI: {translated_text}")
        
        # TTS: í…ìŠ¤íŠ¸ â†’ ìŒì„± ì¶œë ¥
        tts = gTTS(translated_text, lang=LANGUAGES[output_lang], slow=False)
        audio_file = io.BytesIO()
        tts.write_to_fp(audio_file)
        audio_file.seek(0)
        st.audio(audio_file, format='audio/mp3')
        st.info("TTS: gTTS (ë˜ë ·í•œ ì—¬ì„±-like ëª©ì†Œë¦¬). ì—…ë¡œë“œ íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”!")

st.write("ë™ì‹œ í†µì—­ íŒ: ë§ˆì´í¬ ë²„íŠ¼ í´ë¦­ í›„ ë§í•˜ì„¸ìš”. ìŠ¤ë§ˆíŠ¸í°ì—ì„œ ì˜ ë™ì‘í•´ìš” â€“ ì™¸ë¶€ì—ì„œë„ ë°ì´í„°ë¡œ OK!")
